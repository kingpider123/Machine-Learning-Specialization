# INTUITION
Backpropagation is the algorithm used to efficiently compute gradients of the cost function with respect to each parameter in a neural network. Itâ€™s the backbone of training via gradient descent.
```
Instead of computing each gradient independently, backprop uses shared intermediate derivatives, making it scale well:

If a network has ğ‘› nodes and ğ‘ parameters, backprop computes all gradients in roughly ğ‘›+ğ‘ stepsâ€”not ğ‘›Ã—ğ‘.
```
![alt text](img/image-12.png)